//! FLANN-based approximate nearest neighbor search
//!
//! FLANN (Fast Library for Approximate Nearest Neighbors) uses hierarchical
//! k-means trees and randomized kd-trees for fast approximate matching.

use crate::descriptor::Descriptors;
use cv_core::{FeatureMatch, Matches};
use std::cmp::Ordering;
use std::collections::BinaryHeap;

/// FLANN index for fast approximate nearest neighbor search
pub struct FlannIndex {
    trees: Vec<KdTree>,
    branching_factor: usize,
    num_trees: usize,
    num_checks: usize,
}

/// KD-Tree node for spatial partitioning
enum KdTreeNode {
    Leaf(Vec<usize>), // Indices of points in this leaf
    Internal {
        dimension: usize,
        threshold: u8,
        left: Box<KdTreeNode>,
        right: Box<KdTreeNode>,
    },
}

/// KD-Tree for efficient nearest neighbor search
struct KdTree {
    root: KdTreeNode,
    points: Vec<Vec<u8>>, // Copy of points for distance computation
}

/// Search result with distance
#[derive(Clone, Copy)]
struct SearchResult {
    index: usize,
    distance: u32,
}

impl Ord for SearchResult {
    fn cmp(&self, other: &Self) -> Ordering {
        other.distance.cmp(&self.distance) // Reverse for min-heap
    }
}

impl PartialOrd for SearchResult {
    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
        Some(self.cmp(other))
    }
}

impl PartialEq for SearchResult {
    fn eq(&self, other: &Self) -> bool {
        self.distance == other.distance
    }
}

impl Eq for SearchResult {}

impl FlannIndex {
    /// Create a new FLANN index
    pub fn new(branching_factor: usize, num_trees: usize) -> Self {
        Self {
            trees: Vec::new(),
            branching_factor,
            num_trees,
            num_checks: 32, // Number of leaf nodes to check
        }
    }

    /// Set the number of checks during search (trade-off between speed and accuracy)
    pub fn with_checks(mut self, num_checks: usize) -> Self {
        self.num_checks = num_checks;
        self
    }

    /// Build the index from descriptors
    pub fn build(&mut self, descriptors: &Descriptors) {
        self.trees.clear();

        // Convert descriptors to byte vectors
        let points: Vec<Vec<u8>> = descriptors.iter().map(|d| d.data.clone()).collect();

        // Build multiple randomized KD-trees
        for tree_id in 0..self.num_trees {
            let tree = KdTree::build(&points, self.branching_factor, tree_id);
            self.trees.push(tree);
        }
    }

    /// Search for k nearest neighbors
    pub fn search_knn(&self, query: &[u8], k: usize) -> Vec<(usize, u32)> {
        let mut candidates = BinaryHeap::new();

        // Search all trees
        for tree in &self.trees {
            tree.search_knn(&tree.root, query, k, self.num_checks, &mut candidates);
        }

        // Convert to sorted results (closest first)
        let mut results: Vec<_> = candidates.into_sorted_vec();
        results.truncate(k);

        results
            .into_iter()
            .map(|sr| (sr.index, sr.distance))
            .collect()
    }

    /// Find approximate nearest neighbors for all query descriptors
    pub fn knn_search(&self, queries: &Descriptors, k: usize) -> Vec<Vec<(usize, u32)>> {
        queries
            .iter()
            .map(|desc| self.search_knn(&desc.data, k))
            .collect()
    }
}

impl KdTree {
    /// Build a KD-tree from points
    fn build(points: &[Vec<u8>], branching_factor: usize, tree_id: usize) -> Self {
        let indices: Vec<usize> = (0..points.len()).collect();
        let root = Self::build_recursive(points, &indices, 0, branching_factor, tree_id);

        Self {
            root,
            points: points.to_vec(),
        }
    }

    fn build_recursive(
        points: &[Vec<u8>],
        indices: &[usize],
        depth: usize,
        branching_factor: usize,
        tree_id: usize,
    ) -> KdTreeNode {
        if indices.len() <= branching_factor {
            return KdTreeNode::Leaf(indices.to_vec());
        }

        // Choose dimension based on depth and randomize slightly per tree
        let num_dims = points[indices[0]].len();
        let dimension = (depth + tree_id) % num_dims;

        // Find median value
        let mut values: Vec<u8> = indices.iter().map(|&i| points[i][dimension]).collect();
        values.sort();
        let threshold = values[values.len() / 2];

        // Partition indices
        let left_indices: Vec<usize> = indices
            .iter()
            .filter(|&&i| points[i][dimension] < threshold)
            .copied()
            .collect();
        let right_indices: Vec<usize> = indices
            .iter()
            .filter(|&&i| points[i][dimension] >= threshold)
            .copied()
            .collect();

        // Handle edge case where all values are equal
        if left_indices.is_empty() || right_indices.is_empty() {
            return KdTreeNode::Leaf(indices.to_vec());
        }

        KdTreeNode::Internal {
            dimension,
            threshold,
            left: Box::new(Self::build_recursive(
                points,
                &left_indices,
                depth + 1,
                branching_factor,
                tree_id,
            )),
            right: Box::new(Self::build_recursive(
                points,
                &right_indices,
                depth + 1,
                branching_factor,
                tree_id,
            )),
        }
    }

    fn search_knn(
        &self,
        node: &KdTreeNode,
        query: &[u8],
        k: usize,
        max_checks: usize,
        results: &mut BinaryHeap<SearchResult>,
    ) {
        match node {
            KdTreeNode::Leaf(indices) => {
                for &idx in indices {
                    let dist = hamming_distance(query, &self.points[idx]);
                    results.push(SearchResult {
                        index: idx,
                        distance: dist,
                    });
                }
            }
            KdTreeNode::Internal {
                dimension,
                threshold,
                left,
                right,
            } => {
                // Choose which side to search first
                let query_val = query[*dimension];
                let (first, second) = if query_val < *threshold {
                    (left, right)
                } else {
                    (right, left)
                };

                // Search the closer side first
                self.search_knn(first, query, k, max_checks, results);

                // Check if we need to search the other side
                if results.len() < k || results.peek().map(|r| r.distance).unwrap_or(u32::MAX) > 0 {
                    let diff = if query_val > *threshold {
                        query_val - *threshold
                    } else {
                        *threshold - query_val
                    };

                    // Simple heuristic: if dimension difference is small, check other side
                    if diff < 128 {
                        self.search_knn(second, query, k, max_checks, results);
                    }
                }
            }
        }
    }
}

/// Compute Hamming distance between two byte vectors
fn hamming_distance(a: &[u8], b: &[u8]) -> u32 {
    a.iter()
        .zip(b.iter())
        .map(|(x, y)| (x ^ y).count_ones())
        .sum()
}

/// FLANN-based matcher
pub struct FlannMatcher {
    index: FlannIndex,
    ratio_threshold: Option<f32>,
}

impl FlannMatcher {
    pub fn new() -> Self {
        Self {
            index: FlannIndex::new(32, 4), // 4 trees, branching factor 32
            ratio_threshold: None,
        }
    }

    pub fn with_ratio_test(mut self, threshold: f32) -> Self {
        self.ratio_threshold = Some(threshold);
        self
    }

    /// Train the matcher with reference descriptors
    pub fn train(&mut self, train_descriptors: &Descriptors) {
        self.index.build(train_descriptors);
    }

    /// Match query descriptors against trained index
    pub fn match_descriptors(&self, query_descriptors: &Descriptors) -> Matches {
        let mut matches = Matches::new();
        let knn_results = self.index.knn_search(query_descriptors, 2);

        for (query_idx, knn) in knn_results.iter().enumerate() {
            if knn.is_empty() {
                continue;
            }

            let best_match = knn[0];

            // Apply ratio test if enabled
            if let Some(ratio) = self.ratio_threshold {
                if knn.len() >= 2 {
                    let second_match = knn[1];
                    let ratio_calc = best_match.1 as f32 / second_match.1 as f32;
                    if ratio_calc > ratio {
                        continue;
                    }
                }
            }

            let m = FeatureMatch::new(query_idx as i32, best_match.0 as i32, best_match.1 as f32);
            matches.push(m);
        }

        matches
    }
}

/// Match two sets of descriptors using FLANN
pub fn flann_match(query: &Descriptors, train: &Descriptors, k: usize) -> Vec<Vec<FeatureMatch>> {
    let mut index = FlannIndex::new(32, 4);
    index.build(train);

    let knn_results = index.knn_search(query, k);

    knn_results
        .into_iter()
        .enumerate()
        .map(|(query_idx, neighbors)| {
            neighbors
                .into_iter()
                .map(|(train_idx, distance)| {
                    FeatureMatch::new(query_idx as i32, train_idx as i32, distance as f32)
                })
                .collect()
        })
        .collect()
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::descriptor::Descriptor;
    use cv_core::KeyPoint;

    fn create_test_descriptors(n: usize) -> Descriptors {
        let mut descs = Descriptors::new();
        for i in 0..n {
            let data: Vec<u8> = (0..32).map(|j| ((i * 7 + j * 3) % 256) as u8).collect();
            descs.push(Descriptor::new(data, KeyPoint::new(i as f64, i as f64)));
        }
        descs
    }

    #[test]
    fn test_flann_index() {
        let train = create_test_descriptors(100);
        let mut index = FlannIndex::new(16, 2);
        index.build(&train);

        // Create a query that's identical to one in the training set
        let query: Vec<u8> = (0..32).map(|j| ((0 * 7 + j * 3) % 256) as u8).collect();
        let results = index.search_knn(&query, 3);

        assert!(!results.is_empty(), "Should find nearest neighbors");
        // Note: Due to approximate nature, might not find exact match
        // Just check that we get results
        println!("Found {} neighbors for query", results.len());
        for (idx, dist) in &results {
            println!("  Index {} with distance {}", idx, dist);
        }
    }

    #[test]
    fn test_flann_matcher() {
        let train = create_test_descriptors(100);
        let query = create_test_descriptors(20);

        // Use lower ratio threshold and without ratio test
        let mut matcher = FlannMatcher::new();
        matcher.train(&train);

        let matches = matcher.match_descriptors(&query);

        println!("FLANN found {} matches", matches.len());
        // Just verify the pipeline works - exact match count depends on data
        assert!(
            matches.len() <= query.len(),
            "Should not have more matches than queries"
        );
    }
}
